{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Converting songs to matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:00<00:00, 88.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Converted 39 songs to matrix\n",
      "\n",
      "\n",
      "\n",
      "(20, 156)\n",
      "Processing song: 0/144\n",
      "Processing song: 50/144\n",
      "Processing song: 100/144\n",
      "[*] Embedding Songs\n",
      "Processing embed: 0/144\n",
      "Processing embed: 50/144\n",
      "Processing embed: 100/144\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import midi_manipulation\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from distutils.version import LooseVersion\n",
    "from utils import *\n",
    "\n",
    "data_path = './data/edm_essential_melodies'\n",
    "\n",
    "input_songs, target_songs = get_song_matrixes(data_path, 40, 20)\n",
    "\n",
    "input_songs = np.array(input_songs)\n",
    "target_songs = np.array(target_songs)\n",
    "\n",
    "\n",
    "tokens = get_tokens(input_songs)\n",
    "num_encoder_tokens = np.array(tokens).shape[0]\n",
    "num_decoder_tokens = np.array(tokens).shape[0]\n",
    "\n",
    "\n",
    "print('[*] Embedding Songs')\n",
    "embeded_input_songs = get_embeded_songs(input_songs, tokens, num_encoder_tokens)\n",
    "embeded_target_songs = embeded_input_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 839)\n",
      "(20, 156)\n",
      "Number of samples: 144\n",
      "Number of unique input tokens: 839\n",
      "Number of unique output tokens: 839\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 20\n",
      "(144, 20, 839) (144, 20, 839)\n",
      "\n",
      "Encoder input data shape: (144, 20, 839)\n",
      "Decoder input data shape: (144, 20, 839)\n",
      "Decoder target data shape: (144, 20, 839)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(np.array(embeded_input_songs[0]).shape)\n",
    "\n",
    "\n",
    "print(np.array(embed_song_to_song(embeded_input_songs[0], tokens)).shape)\n",
    "\n",
    "# Finding the longest song in the dataset\n",
    "max_encoder_seq_length = max([len(song) for song in embeded_input_songs])\n",
    "max_decoder_seq_length = max([len(song) for song in embeded_target_songs])\n",
    "\n",
    "\n",
    "print('Number of samples:', len(embeded_input_songs))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "\n",
    "# Get input data in shape (num_sample, max_seq_length, num_tokens)\n",
    "encoder_input_data, decoder_input_data, decoder_target_data = get_input_data(\n",
    "                                                                        embeded_input_songs, \n",
    "                                                                        embeded_target_songs,\n",
    "                                                                        max_encoder_seq_length, \n",
    "                                                                        num_encoder_tokens, \n",
    "                                                                        max_decoder_seq_length, \n",
    "                                                                        num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, None, 839)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, None, 839)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, None, 512),  2768896     input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, None, 512),  2768896     input_20[0][0]                   \n",
      "                                                                 lstm_11[0][1]                    \n",
      "                                                                 lstm_11[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 839)    430407      lstm_12[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,968,199\n",
      "Trainable params: 5,968,199\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size  =   16      # Batch size for training.\n",
    "epochs      =   120     # Number of epochs to train for.\n",
    "latent_dim  =   512     # Latent dimensionality of the encoding space.\n",
    "#num_samples =   10000   # Number of samples to train on.\n",
    "\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print()\n",
    "print(model.summary())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Starting Training\n",
      "Epoch 1/120\n",
      "144/144 [==============================] - ETA: 16s - loss: 7.06 - ETA: 7s - loss: 7.0633 - ETA: 4s - loss: 7.059 - ETA: 2s - loss: 7.050 - ETA: 1s - loss: 7.039 - ETA: 1s - loss: 7.013 - ETA: 0s - loss: 6.939 - ETA: 0s - loss: 6.970 - 3s 18ms/step - loss: 6.8816\n",
      "Epoch 2/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 6.325 - ETA: 0s - loss: 6.384 - ETA: 0s - loss: 6.360 - ETA: 0s - loss: 6.271 - ETA: 0s - loss: 6.273 - ETA: 0s - loss: 6.262 - ETA: 0s - loss: 6.270 - ETA: 0s - loss: 6.212 - 1s 4ms/step - loss: 6.1981\n",
      "Epoch 3/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 6.103 - ETA: 0s - loss: 6.155 - ETA: 0s - loss: 5.998 - ETA: 0s - loss: 6.055 - ETA: 0s - loss: 6.001 - ETA: 0s - loss: 6.060 - ETA: 0s - loss: 6.038 - ETA: 0s - loss: 6.039 - 1s 4ms/step - loss: 6.0184\n",
      "Epoch 4/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 6.188 - ETA: 0s - loss: 5.903 - ETA: 0s - loss: 6.069 - ETA: 0s - loss: 5.939 - ETA: 0s - loss: 5.979 - ETA: 0s - loss: 5.930 - ETA: 0s - loss: 5.939 - ETA: 0s - loss: 5.864 - 1s 4ms/step - loss: 5.8389\n",
      "Epoch 5/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.548 - ETA: 0s - loss: 5.519 - ETA: 0s - loss: 5.615 - ETA: 0s - loss: 5.674 - ETA: 0s - loss: 5.699 - ETA: 0s - loss: 5.737 - ETA: 0s - loss: 5.713 - ETA: 0s - loss: 5.697 - 1s 4ms/step - loss: 5.7102\n",
      "Epoch 6/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.908 - ETA: 0s - loss: 5.760 - ETA: 0s - loss: 5.738 - ETA: 0s - loss: 5.702 - ETA: 0s - loss: 5.624 - ETA: 0s - loss: 5.627 - ETA: 0s - loss: 5.652 - ETA: 0s - loss: 5.617 - 1s 4ms/step - loss: 5.6057\n",
      "Epoch 7/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.338 - ETA: 0s - loss: 5.282 - ETA: 0s - loss: 5.184 - ETA: 0s - loss: 5.267 - ETA: 0s - loss: 5.315 - ETA: 0s - loss: 5.401 - ETA: 0s - loss: 5.469 - ETA: 0s - loss: 5.493 - 1s 4ms/step - loss: 5.5144\n",
      "Epoch 8/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.248 - ETA: 0s - loss: 5.380 - ETA: 0s - loss: 5.387 - ETA: 0s - loss: 5.397 - ETA: 0s - loss: 5.401 - ETA: 0s - loss: 5.467 - ETA: 0s - loss: 5.415 - ETA: 0s - loss: 5.420 - 1s 4ms/step - loss: 5.4100\n",
      "Epoch 9/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.957 - ETA: 0s - loss: 5.023 - ETA: 0s - loss: 5.087 - ETA: 0s - loss: 5.171 - ETA: 0s - loss: 5.259 - ETA: 0s - loss: 5.234 - ETA: 0s - loss: 5.295 - ETA: 0s - loss: 5.297 - 1s 4ms/step - loss: 5.2885\n",
      "Epoch 10/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.520 - ETA: 0s - loss: 4.843 - ETA: 0s - loss: 4.983 - ETA: 0s - loss: 5.015 - ETA: 0s - loss: 5.082 - ETA: 0s - loss: 5.094 - ETA: 0s - loss: 5.106 - ETA: 0s - loss: 5.112 - 1s 4ms/step - loss: 5.1861\n",
      "Epoch 11/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.466 - ETA: 0s - loss: 5.198 - ETA: 0s - loss: 5.167 - ETA: 0s - loss: 5.055 - ETA: 0s - loss: 5.098 - ETA: 0s - loss: 5.101 - ETA: 0s - loss: 5.112 - ETA: 0s - loss: 5.103 - 1s 4ms/step - loss: 5.0892\n",
      "Epoch 12/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.168 - ETA: 0s - loss: 5.129 - ETA: 0s - loss: 5.058 - ETA: 0s - loss: 4.958 - ETA: 0s - loss: 4.947 - ETA: 0s - loss: 4.969 - ETA: 0s - loss: 4.956 - ETA: 0s - loss: 4.985 - 1s 4ms/step - loss: 5.0157\n",
      "Epoch 13/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.268 - ETA: 0s - loss: 5.046 - ETA: 0s - loss: 5.057 - ETA: 0s - loss: 5.032 - ETA: 0s - loss: 5.015 - ETA: 0s - loss: 4.949 - ETA: 0s - loss: 4.941 - ETA: 0s - loss: 4.922 - 1s 4ms/step - loss: 4.9349\n",
      "Epoch 14/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 5.056 - ETA: 0s - loss: 4.860 - ETA: 0s - loss: 4.954 - ETA: 0s - loss: 4.819 - ETA: 0s - loss: 4.836 - ETA: 0s - loss: 4.850 - ETA: 0s - loss: 4.857 - ETA: 0s - loss: 4.872 - 1s 4ms/step - loss: 4.8678\n",
      "Epoch 15/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.426 - ETA: 0s - loss: 4.526 - ETA: 0s - loss: 4.715 - ETA: 0s - loss: 4.764 - ETA: 0s - loss: 4.742 - ETA: 0s - loss: 4.797 - ETA: 0s - loss: 4.815 - ETA: 0s - loss: 4.830 - 1s 4ms/step - loss: 4.8487\n",
      "Epoch 16/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.865 - ETA: 0s - loss: 4.870 - ETA: 0s - loss: 4.735 - ETA: 0s - loss: 4.744 - ETA: 0s - loss: 4.737 - ETA: 0s - loss: 4.711 - ETA: 0s - loss: 4.723 - ETA: 0s - loss: 4.766 - 1s 4ms/step - loss: 4.7986\n",
      "Epoch 17/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.554 - ETA: 0s - loss: 4.669 - ETA: 0s - loss: 4.543 - ETA: 0s - loss: 4.656 - ETA: 0s - loss: 4.628 - ETA: 0s - loss: 4.670 - ETA: 0s - loss: 4.649 - ETA: 0s - loss: 4.651 - 1s 4ms/step - loss: 4.6675\n",
      "Epoch 18/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.410 - ETA: 0s - loss: 4.466 - ETA: 0s - loss: 4.453 - ETA: 0s - loss: 4.417 - ETA: 0s - loss: 4.378 - ETA: 0s - loss: 4.425 - ETA: 0s - loss: 4.487 - ETA: 0s - loss: 4.486 - 1s 4ms/step - loss: 4.5004\n",
      "Epoch 19/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.360 - ETA: 0s - loss: 4.269 - ETA: 0s - loss: 4.321 - ETA: 0s - loss: 4.380 - ETA: 0s - loss: 4.413 - ETA: 0s - loss: 4.371 - ETA: 0s - loss: 4.389 - ETA: 0s - loss: 4.348 - 1s 4ms/step - loss: 4.3449\n",
      "Epoch 20/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.095 - ETA: 0s - loss: 4.188 - ETA: 0s - loss: 4.130 - ETA: 0s - loss: 4.088 - ETA: 0s - loss: 4.100 - ETA: 0s - loss: 4.127 - ETA: 0s - loss: 4.165 - ETA: 0s - loss: 4.226 - 1s 4ms/step - loss: 4.2600\n",
      "Epoch 21/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 4.562 - ETA: 0s - loss: 4.323 - ETA: 0s - loss: 4.170 - ETA: 0s - loss: 4.173 - ETA: 0s - loss: 4.092 - ETA: 0s - loss: 4.120 - ETA: 0s - loss: 4.129 - ETA: 0s - loss: 4.107 - 1s 4ms/step - loss: 4.1221\n",
      "Epoch 22/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.826 - ETA: 0s - loss: 4.018 - ETA: 0s - loss: 4.053 - ETA: 0s - loss: 4.023 - ETA: 0s - loss: 4.068 - ETA: 0s - loss: 4.075 - ETA: 0s - loss: 4.081 - ETA: 0s - loss: 4.098 - 1s 4ms/step - loss: 4.0838\n",
      "Epoch 23/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.616 - ETA: 0s - loss: 3.699 - ETA: 0s - loss: 3.737 - ETA: 0s - loss: 3.797 - ETA: 0s - loss: 3.893 - ETA: 0s - loss: 3.923 - ETA: 0s - loss: 3.974 - ETA: 0s - loss: 3.950 - 1s 4ms/step - loss: 3.9729\n",
      "Epoch 24/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.618 - ETA: 0s - loss: 3.703 - ETA: 0s - loss: 3.617 - ETA: 0s - loss: 3.654 - ETA: 0s - loss: 3.734 - ETA: 0s - loss: 3.752 - ETA: 0s - loss: 3.773 - ETA: 0s - loss: 3.821 - 1s 4ms/step - loss: 3.8584\n",
      "Epoch 25/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.505 - ETA: 0s - loss: 3.697 - ETA: 0s - loss: 3.637 - ETA: 0s - loss: 3.721 - ETA: 0s - loss: 3.690 - ETA: 0s - loss: 3.683 - ETA: 0s - loss: 3.703 - ETA: 0s - loss: 3.759 - 1s 4ms/step - loss: 3.7532\n",
      "Epoch 26/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.232 - ETA: 0s - loss: 3.449 - ETA: 0s - loss: 3.446 - ETA: 0s - loss: 3.465 - ETA: 0s - loss: 3.469 - ETA: 0s - loss: 3.515 - ETA: 0s - loss: 3.540 - ETA: 0s - loss: 3.595 - 1s 4ms/step - loss: 3.6469\n",
      "Epoch 27/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.461 - ETA: 0s - loss: 3.396 - ETA: 0s - loss: 3.423 - ETA: 0s - loss: 3.412 - ETA: 0s - loss: 3.493 - ETA: 0s - loss: 3.467 - ETA: 0s - loss: 3.488 - ETA: 0s - loss: 3.520 - 1s 4ms/step - loss: 3.5289\n",
      "Epoch 28/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.411 - ETA: 0s - loss: 3.457 - ETA: 0s - loss: 3.515 - ETA: 0s - loss: 3.466 - ETA: 0s - loss: 3.510 - ETA: 0s - loss: 3.494 - ETA: 0s - loss: 3.483 - ETA: 0s - loss: 3.466 - 1s 4ms/step - loss: 3.4430\n",
      "Epoch 29/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 3.326 - ETA: 0s - loss: 3.291 - ETA: 0s - loss: 3.229 - ETA: 0s - loss: 3.245 - ETA: 0s - loss: 3.307 - ETA: 0s - loss: 3.344 - ETA: 0s - loss: 3.340 - ETA: 0s - loss: 3.321 - 1s 4ms/step - loss: 3.3168\n",
      "Epoch 30/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - ETA: 0s - loss: 3.397 - ETA: 0s - loss: 3.324 - ETA: 0s - loss: 3.179 - ETA: 0s - loss: 3.168 - ETA: 0s - loss: 3.066 - ETA: 0s - loss: 3.080 - ETA: 0s - loss: 3.082 - ETA: 0s - loss: 3.060 - 1s 4ms/step - loss: 3.0763\n",
      "Epoch 31/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.885 - ETA: 0s - loss: 2.866 - ETA: 0s - loss: 2.872 - ETA: 0s - loss: 2.859 - ETA: 0s - loss: 2.859 - ETA: 0s - loss: 2.873 - ETA: 0s - loss: 2.867 - ETA: 0s - loss: 2.862 - 1s 4ms/step - loss: 2.8654\n",
      "Epoch 32/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.926 - ETA: 0s - loss: 2.747 - ETA: 0s - loss: 2.732 - ETA: 0s - loss: 2.732 - ETA: 0s - loss: 2.732 - ETA: 0s - loss: 2.740 - ETA: 0s - loss: 2.724 - ETA: 0s - loss: 2.699 - 1s 4ms/step - loss: 2.6753\n",
      "Epoch 33/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.416 - ETA: 0s - loss: 2.400 - ETA: 0s - loss: 2.395 - ETA: 0s - loss: 2.450 - ETA: 0s - loss: 2.463 - ETA: 0s - loss: 2.490 - ETA: 0s - loss: 2.501 - ETA: 0s - loss: 2.526 - 1s 4ms/step - loss: 2.5396\n",
      "Epoch 34/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.541 - ETA: 0s - loss: 2.489 - ETA: 0s - loss: 2.386 - ETA: 0s - loss: 2.391 - ETA: 0s - loss: 2.381 - ETA: 0s - loss: 2.426 - ETA: 0s - loss: 2.430 - ETA: 0s - loss: 2.431 - 1s 4ms/step - loss: 2.4510\n",
      "Epoch 35/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.375 - ETA: 0s - loss: 2.296 - ETA: 0s - loss: 2.372 - ETA: 0s - loss: 2.434 - ETA: 0s - loss: 2.429 - ETA: 0s - loss: 2.388 - ETA: 0s - loss: 2.363 - ETA: 0s - loss: 2.366 - 1s 4ms/step - loss: 2.3813\n",
      "Epoch 36/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.142 - ETA: 0s - loss: 2.223 - ETA: 0s - loss: 2.253 - ETA: 0s - loss: 2.233 - ETA: 0s - loss: 2.239 - ETA: 0s - loss: 2.231 - ETA: 0s - loss: 2.269 - ETA: 0s - loss: 2.272 - 1s 4ms/step - loss: 2.2751\n",
      "Epoch 37/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.135 - ETA: 0s - loss: 2.058 - ETA: 0s - loss: 2.080 - ETA: 0s - loss: 2.071 - ETA: 0s - loss: 2.116 - ETA: 0s - loss: 2.131 - ETA: 0s - loss: 2.146 - ETA: 0s - loss: 2.148 - 1s 4ms/step - loss: 2.1535\n",
      "Epoch 38/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.182 - ETA: 0s - loss: 2.144 - ETA: 0s - loss: 2.103 - ETA: 0s - loss: 2.051 - ETA: 0s - loss: 2.035 - ETA: 0s - loss: 2.067 - ETA: 0s - loss: 2.065 - ETA: 0s - loss: 2.056 - 1s 4ms/step - loss: 2.0778\n",
      "Epoch 39/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.954 - ETA: 0s - loss: 2.051 - ETA: 0s - loss: 2.048 - ETA: 0s - loss: 2.024 - ETA: 0s - loss: 2.020 - ETA: 0s - loss: 2.062 - ETA: 0s - loss: 2.051 - ETA: 0s - loss: 2.036 - 1s 4ms/step - loss: 2.0550\n",
      "Epoch 40/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 2.006 - ETA: 0s - loss: 1.957 - ETA: 0s - loss: 1.972 - ETA: 0s - loss: 2.042 - ETA: 0s - loss: 2.035 - ETA: 0s - loss: 2.008 - ETA: 0s - loss: 1.967 - ETA: 0s - loss: 1.934 - 1s 4ms/step - loss: 1.9536\n",
      "Epoch 41/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.805 - ETA: 0s - loss: 1.850 - ETA: 0s - loss: 1.883 - ETA: 0s - loss: 1.854 - ETA: 0s - loss: 1.872 - ETA: 0s - loss: 1.870 - ETA: 0s - loss: 1.846 - ETA: 0s - loss: 1.843 - 1s 4ms/step - loss: 1.8715\n",
      "Epoch 42/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.741 - ETA: 0s - loss: 1.784 - ETA: 0s - loss: 1.792 - ETA: 0s - loss: 1.753 - ETA: 0s - loss: 1.740 - ETA: 0s - loss: 1.749 - ETA: 0s - loss: 1.724 - ETA: 0s - loss: 1.735 - 1s 4ms/step - loss: 1.7347\n",
      "Epoch 43/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.745 - ETA: 0s - loss: 1.683 - ETA: 0s - loss: 1.693 - ETA: 0s - loss: 1.664 - ETA: 0s - loss: 1.612 - ETA: 0s - loss: 1.604 - ETA: 0s - loss: 1.618 - ETA: 0s - loss: 1.642 - 1s 4ms/step - loss: 1.6410\n",
      "Epoch 44/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.403 - ETA: 0s - loss: 1.494 - ETA: 0s - loss: 1.546 - ETA: 0s - loss: 1.567 - ETA: 0s - loss: 1.565 - ETA: 0s - loss: 1.543 - ETA: 0s - loss: 1.546 - ETA: 0s - loss: 1.542 - 1s 4ms/step - loss: 1.5521\n",
      "Epoch 45/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.472 - ETA: 0s - loss: 1.593 - ETA: 0s - loss: 1.562 - ETA: 0s - loss: 1.511 - ETA: 0s - loss: 1.503 - ETA: 0s - loss: 1.487 - ETA: 0s - loss: 1.526 - ETA: 0s - loss: 1.506 - 1s 4ms/step - loss: 1.5128\n",
      "Epoch 46/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.267 - ETA: 0s - loss: 1.348 - ETA: 0s - loss: 1.354 - ETA: 0s - loss: 1.396 - ETA: 0s - loss: 1.434 - ETA: 0s - loss: 1.412 - ETA: 0s - loss: 1.429 - ETA: 0s - loss: 1.443 - 1s 4ms/step - loss: 1.4495\n",
      "Epoch 47/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.230 - ETA: 0s - loss: 1.340 - ETA: 0s - loss: 1.328 - ETA: 0s - loss: 1.352 - ETA: 0s - loss: 1.347 - ETA: 0s - loss: 1.351 - ETA: 0s - loss: 1.358 - ETA: 0s - loss: 1.338 - 1s 4ms/step - loss: 1.3582\n",
      "Epoch 48/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.204 - ETA: 0s - loss: 1.246 - ETA: 0s - loss: 1.270 - ETA: 0s - loss: 1.242 - ETA: 0s - loss: 1.247 - ETA: 0s - loss: 1.245 - ETA: 0s - loss: 1.271 - ETA: 0s - loss: 1.261 - 1s 4ms/step - loss: 1.2744\n",
      "Epoch 49/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.182 - ETA: 0s - loss: 1.307 - ETA: 0s - loss: 1.273 - ETA: 0s - loss: 1.252 - ETA: 0s - loss: 1.230 - ETA: 0s - loss: 1.250 - ETA: 0s - loss: 1.222 - ETA: 0s - loss: 1.219 - 1s 4ms/step - loss: 1.2203\n",
      "Epoch 50/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.228 - ETA: 0s - loss: 1.205 - ETA: 0s - loss: 1.191 - ETA: 0s - loss: 1.180 - ETA: 0s - loss: 1.159 - ETA: 0s - loss: 1.156 - ETA: 0s - loss: 1.166 - ETA: 0s - loss: 1.181 - 1s 4ms/step - loss: 1.1807\n",
      "Epoch 51/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.070 - ETA: 0s - loss: 1.059 - ETA: 0s - loss: 1.135 - ETA: 0s - loss: 1.103 - ETA: 0s - loss: 1.119 - ETA: 0s - loss: 1.141 - ETA: 0s - loss: 1.125 - ETA: 0s - loss: 1.107 - 1s 4ms/step - loss: 1.0977\n",
      "Epoch 52/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.047 - ETA: 0s - loss: 0.968 - ETA: 0s - loss: 0.974 - ETA: 0s - loss: 0.987 - ETA: 0s - loss: 0.984 - ETA: 0s - loss: 0.999 - ETA: 0s - loss: 0.995 - ETA: 0s - loss: 1.031 - 1s 4ms/step - loss: 1.0305\n",
      "Epoch 53/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.090 - ETA: 0s - loss: 0.985 - ETA: 0s - loss: 1.036 - ETA: 0s - loss: 0.984 - ETA: 0s - loss: 0.946 - ETA: 0s - loss: 0.982 - ETA: 0s - loss: 0.966 - ETA: 0s - loss: 0.970 - 1s 4ms/step - loss: 0.9760\n",
      "Epoch 54/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.030 - ETA: 0s - loss: 0.994 - ETA: 0s - loss: 0.919 - ETA: 0s - loss: 0.913 - ETA: 0s - loss: 0.898 - ETA: 0s - loss: 0.905 - ETA: 0s - loss: 0.911 - ETA: 0s - loss: 0.930 - 1s 4ms/step - loss: 0.9273\n",
      "Epoch 55/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 1.074 - ETA: 0s - loss: 0.913 - ETA: 0s - loss: 0.863 - ETA: 0s - loss: 0.870 - ETA: 0s - loss: 0.861 - ETA: 0s - loss: 0.876 - ETA: 0s - loss: 0.876 - ETA: 0s - loss: 0.866 - 1s 4ms/step - loss: 0.8743\n",
      "Epoch 56/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.854 - ETA: 0s - loss: 0.878 - ETA: 0s - loss: 0.888 - ETA: 0s - loss: 0.835 - ETA: 0s - loss: 0.828 - ETA: 0s - loss: 0.819 - ETA: 0s - loss: 0.809 - ETA: 0s - loss: 0.831 - 1s 4ms/step - loss: 0.8289\n",
      "Epoch 57/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.815 - ETA: 0s - loss: 0.799 - ETA: 0s - loss: 0.810 - ETA: 0s - loss: 0.815 - ETA: 0s - loss: 0.792 - ETA: 0s - loss: 0.816 - ETA: 0s - loss: 0.829 - ETA: 0s - loss: 0.810 - 1s 4ms/step - loss: 0.8024\n",
      "Epoch 58/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.629 - ETA: 0s - loss: 0.728 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.722 - ETA: 0s - loss: 0.747 - ETA: 0s - loss: 0.777 - ETA: 0s - loss: 0.787 - 1s 4ms/step - loss: 0.7949\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.748 - ETA: 0s - loss: 0.747 - ETA: 0s - loss: 0.728 - ETA: 0s - loss: 0.787 - ETA: 0s - loss: 0.774 - ETA: 0s - loss: 0.754 - ETA: 0s - loss: 0.751 - 1s 4ms/step - loss: 0.7588\n",
      "Epoch 60/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.761 - ETA: 0s - loss: 0.802 - ETA: 0s - loss: 0.805 - ETA: 0s - loss: 0.789 - ETA: 0s - loss: 0.733 - ETA: 0s - loss: 0.729 - ETA: 0s - loss: 0.710 - ETA: 0s - loss: 0.726 - 1s 4ms/step - loss: 0.7253\n",
      "Epoch 61/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.681 - ETA: 0s - loss: 0.638 - ETA: 0s - loss: 0.702 - ETA: 0s - loss: 0.713 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.719 - ETA: 0s - loss: 0.706 - 1s 4ms/step - loss: 0.7061\n",
      "Epoch 62/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.667 - ETA: 0s - loss: 0.610 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.702 - ETA: 0s - loss: 0.696 - ETA: 0s - loss: 0.672 - 1s 4ms/step - loss: 0.6591\n",
      "Epoch 63/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.636 - ETA: 0s - loss: 0.645 - ETA: 0s - loss: 0.536 - ETA: 0s - loss: 0.583 - ETA: 0s - loss: 0.614 - ETA: 0s - loss: 0.625 - ETA: 0s - loss: 0.626 - ETA: 0s - loss: 0.634 - 1s 4ms/step - loss: 0.6447\n",
      "Epoch 64/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.634 - ETA: 0s - loss: 0.777 - ETA: 0s - loss: 0.841 - ETA: 0s - loss: 0.773 - ETA: 0s - loss: 0.699 - ETA: 0s - loss: 0.685 - ETA: 0s - loss: 0.694 - ETA: 0s - loss: 0.672 - 1s 4ms/step - loss: 0.6952\n",
      "Epoch 65/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.476 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.735 - ETA: 0s - loss: 0.701 - ETA: 0s - loss: 0.730 - ETA: 0s - loss: 0.757 - ETA: 0s - loss: 0.724 - ETA: 0s - loss: 0.700 - 1s 4ms/step - loss: 0.6999\n",
      "Epoch 66/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.540 - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.595 - ETA: 0s - loss: 0.633 - ETA: 0s - loss: 0.627 - ETA: 0s - loss: 0.621 - ETA: 0s - loss: 0.636 - 1s 4ms/step - loss: 0.6335\n",
      "Epoch 67/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.512 - ETA: 0s - loss: 0.583 - ETA: 0s - loss: 0.587 - ETA: 0s - loss: 0.536 - ETA: 0s - loss: 0.597 - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.612 - ETA: 0s - loss: 0.615 - 1s 4ms/step - loss: 0.5927\n",
      "Epoch 68/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.667 - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.615 - ETA: 0s - loss: 0.580 - ETA: 0s - loss: 0.586 - ETA: 0s - loss: 0.560 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.559 - 1s 4ms/step - loss: 0.5618\n",
      "Epoch 69/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.737 - ETA: 0s - loss: 0.632 - ETA: 0s - loss: 0.603 - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.569 - ETA: 0s - loss: 0.558 - ETA: 0s - loss: 0.554 - ETA: 0s - loss: 0.540 - 1s 4ms/step - loss: 0.5314\n",
      "Epoch 70/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.463 - ETA: 0s - loss: 0.458 - ETA: 0s - loss: 0.496 - ETA: 0s - loss: 0.529 - ETA: 0s - loss: 0.539 - ETA: 0s - loss: 0.535 - ETA: 0s - loss: 0.510 - ETA: 0s - loss: 0.517 - 1s 4ms/step - loss: 0.5160\n",
      "Epoch 71/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.498 - ETA: 0s - loss: 0.532 - ETA: 0s - loss: 0.523 - ETA: 0s - loss: 0.495 - ETA: 0s - loss: 0.486 - ETA: 0s - loss: 0.459 - ETA: 0s - loss: 0.474 - 1s 4ms/step - loss: 0.4981\n",
      "Epoch 72/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.472 - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.449 - ETA: 0s - loss: 0.469 - ETA: 0s - loss: 0.490 - ETA: 0s - loss: 0.473 - ETA: 0s - loss: 0.471 - ETA: 0s - loss: 0.460 - 1s 4ms/step - loss: 0.4707\n",
      "Epoch 73/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.395 - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.429 - ETA: 0s - loss: 0.451 - ETA: 0s - loss: 0.461 - ETA: 0s - loss: 0.467 - ETA: 0s - loss: 0.454 - ETA: 0s - loss: 0.450 - 1s 4ms/step - loss: 0.4500\n",
      "Epoch 74/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.441 - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.399 - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.442 - ETA: 0s - loss: 0.430 - 1s 4ms/step - loss: 0.4328\n",
      "Epoch 75/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.478 - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.439 - ETA: 0s - loss: 0.435 - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.426 - 1s 4ms/step - loss: 0.4221\n",
      "Epoch 76/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.379 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.428 - ETA: 0s - loss: 0.418 - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.404 - ETA: 0s - loss: 0.399 - 1s 4ms/step - loss: 0.4050\n",
      "Epoch 77/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.425 - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.392 - ETA: 0s - loss: 0.394 - ETA: 0s - loss: 0.424 - ETA: 0s - loss: 0.414 - ETA: 0s - loss: 0.402 - 1s 4ms/step - loss: 0.3932\n",
      "Epoch 78/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.349 - ETA: 0s - loss: 0.362 - ETA: 0s - loss: 0.382 - ETA: 0s - loss: 0.374 - 1s 4ms/step - loss: 0.3764\n",
      "Epoch 79/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.369 - ETA: 0s - loss: 0.384 - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.373 - ETA: 0s - loss: 0.363 - ETA: 0s - loss: 0.360 - 1s 4ms/step - loss: 0.3599\n",
      "Epoch 80/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.380 - ETA: 0s - loss: 0.415 - ETA: 0s - loss: 0.385 - ETA: 0s - loss: 0.355 - ETA: 0s - loss: 0.359 - ETA: 0s - loss: 0.350 - ETA: 0s - loss: 0.344 - ETA: 0s - loss: 0.343 - 1s 4ms/step - loss: 0.3607\n",
      "Epoch 81/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.455 - ETA: 0s - loss: 0.412 - ETA: 0s - loss: 0.400 - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.375 - ETA: 0s - loss: 0.365 - ETA: 0s - loss: 0.369 - ETA: 0s - loss: 0.359 - 1s 4ms/step - loss: 0.3507\n",
      "Epoch 82/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.273 - ETA: 0s - loss: 0.281 - ETA: 0s - loss: 0.285 - ETA: 0s - loss: 0.305 - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.338 - 1s 4ms/step - loss: 0.3418\n",
      "Epoch 83/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.329 - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.321 - 1s 4ms/step - loss: 0.3153\n",
      "Epoch 84/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.259 - ETA: 0s - loss: 0.337 - ETA: 0s - loss: 0.306 - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.302 - 1s 4ms/step - loss: 0.3006\n",
      "Epoch 85/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.328 - ETA: 0s - loss: 0.348 - ETA: 0s - loss: 0.317 - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.300 - ETA: 0s - loss: 0.290 - 1s 4ms/step - loss: 0.2872\n",
      "Epoch 86/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.410 - ETA: 0s - loss: 0.311 - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.284 - 1s 4ms/step - loss: 0.2818\n",
      "Epoch 87/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.312 - ETA: 0s - loss: 0.286 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.282 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.276 - ETA: 0s - loss: 0.277 - 1s 4ms/step - loss: 0.2707\n",
      "Epoch 88/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.268 - ETA: 0s - loss: 0.302 - ETA: 0s - loss: 0.296 - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.286 - 1s 4ms/step - loss: 0.2857\n",
      "Epoch 89/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.330 - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.295 - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.321 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.304 - ETA: 0s - loss: 0.302 - 1s 4ms/step - loss: 0.3248\n",
      "Epoch 90/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.338 - ETA: 0s - loss: 0.287 - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.272 - ETA: 0s - loss: 0.279 - ETA: 0s - loss: 0.298 - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.319 - 1s 4ms/step - loss: 0.3189\n",
      "Epoch 91/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.291 - ETA: 0s - loss: 0.324 - ETA: 0s - loss: 0.389 - ETA: 0s - loss: 0.345 - ETA: 0s - loss: 0.326 - ETA: 0s - loss: 0.303 - ETA: 0s - loss: 0.294 - ETA: 0s - loss: 0.309 - 1s 4ms/step - loss: 0.3087\n",
      "Epoch 92/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.307 - ETA: 0s - loss: 0.283 - ETA: 0s - loss: 0.293 - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.297 - ETA: 0s - loss: 0.289 - ETA: 0s - loss: 0.277 - 1s 4ms/step - loss: 0.2772\n",
      "Epoch 93/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.208 - ETA: 0s - loss: 0.212 - ETA: 0s - loss: 0.204 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.251 - ETA: 0s - loss: 0.254 - ETA: 0s - loss: 0.261 - ETA: 0s - loss: 0.271 - 1s 4ms/step - loss: 0.2590\n",
      "Epoch 94/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.232 - ETA: 0s - loss: 0.314 - ETA: 0s - loss: 0.292 - ETA: 0s - loss: 0.262 - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.249 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.243 - 1s 4ms/step - loss: 0.2455\n",
      "Epoch 95/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.206 - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.224 - ETA: 0s - loss: 0.223 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.233 - 1s 4ms/step - loss: 0.2340\n",
      "Epoch 96/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.248 - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.271 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.244 - ETA: 0s - loss: 0.240 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.236 - 1s 4ms/step - loss: 0.2276\n",
      "Epoch 97/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.241 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.218 - ETA: 0s - loss: 0.222 - ETA: 0s - loss: 0.221 - ETA: 0s - loss: 0.226 - ETA: 0s - loss: 0.227 - 1s 4ms/step - loss: 0.2220\n",
      "Epoch 98/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.217 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.202 - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.205 - ETA: 0s - loss: 0.207 - 1s 4ms/step - loss: 0.2072\n",
      "Epoch 99/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.140 - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.207 - ETA: 0s - loss: 0.209 - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.201 - ETA: 0s - loss: 0.203 - ETA: 0s - loss: 0.201 - 1s 4ms/step - loss: 0.2006\n",
      "Epoch 100/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.200 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.179 - 1s 4ms/step - loss: 0.1879\n",
      "Epoch 101/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.220 - ETA: 0s - loss: 0.187 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.189 - 1s 4ms/step - loss: 0.1834\n",
      "Epoch 102/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.176 - 1s 4ms/step - loss: 0.1770\n",
      "Epoch 103/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.168 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.175 - ETA: 0s - loss: 0.173 - ETA: 0s - loss: 0.172 - 1s 4ms/step - loss: 0.1744\n",
      "Epoch 104/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.170 - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.174 - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.167 - 1s 4ms/step - loss: 0.1685\n",
      "Epoch 105/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.163 - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.169 - ETA: 0s - loss: 0.166 - 1s 4ms/step - loss: 0.1637\n",
      "Epoch 106/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.162 - 1s 4ms/step - loss: 0.1600\n",
      "Epoch 107/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.158 - 1s 4ms/step - loss: 0.1578\n",
      "Epoch 108/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.155 - 1s 4ms/step - loss: 0.1548\n",
      "Epoch 109/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.157 - 1s 4ms/step - loss: 0.1566\n",
      "Epoch 110/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.152 - 1s 4ms/step - loss: 0.1521\n",
      "Epoch 111/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.150 - 1s 4ms/step - loss: 0.1484\n",
      "Epoch 112/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.167 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.150 - 1s 4ms/step - loss: 0.1463\n",
      "Epoch 113/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.145 - 1s 4ms/step - loss: 0.1470\n",
      "Epoch 114/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.150 - 1s 4ms/step - loss: 0.1511\n",
      "Epoch 115/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.158 - 1s 4ms/step - loss: 0.1647\n",
      "Epoch 116/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.172 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.160 - ETA: 0s - loss: 0.162 - ETA: 0s - loss: 0.165 - 1s 4ms/step - loss: 0.1707\n",
      "Epoch 117/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - ETA: 0s - loss: 0.129 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.154 - 1s 4ms/step - loss: 0.1592\n",
      "Epoch 118/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.131 - ETA: 0s - loss: 0.134 - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.151 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.149 - 1s 4ms/step - loss: 0.1493\n",
      "Epoch 119/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.126 - ETA: 0s - loss: 0.130 - ETA: 0s - loss: 0.127 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.137 - ETA: 0s - loss: 0.138 - ETA: 0s - loss: 0.138 - 1s 4ms/step - loss: 0.1408\n",
      "Epoch 120/120\n",
      "144/144 [==============================] - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.139 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.133 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.136 - ETA: 0s - loss: 0.136 - 1s 4ms/step - loss: 0.1378\n",
      "[*] Ready to be used \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "print('[*] Starting Training')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)\n",
    "# model.load_weights('s2s'+data_path+'.h5')\n",
    "print('[*] Ready to be used \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damygame\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:2344: UserWarning: Layer lstm_12 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_11/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_11/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('s2s'+'edm'+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20, 156)\n",
      "Processing embed: 0/3\n",
      "[*] Encoding-Decoding\n",
      "(2, 2, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "[*] Converting and saving song\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    print(np.array(states_value).shape)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    song_matrix = np.zeros(\n",
    "                        (max_decoder_seq_length, \n",
    "                        num_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        idx = np.argmax(output_tokens[-1,-1,:])\n",
    "        #print(output_tokens[-1,-1,:])\n",
    "        song_matrix[i, idx] = 1\n",
    "        target_seq[0, 0, idx] = 1.\n",
    "        #print(np.array(h).shape)\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (i+2 > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        \n",
    "        \n",
    "\n",
    "        if i%250 == 0:\n",
    "            print('[iter:{}] [max_decoder_seq_length: {}]'.format(i, max_decoder_seq_length))\n",
    "        i+=1\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return song_matrix\n",
    "\n",
    "\n",
    "seq_length = max_encoder_seq_length\n",
    "\n",
    "song = np.array(midi_manipulation.midiToNoteStateMatrix('./data/edm_essential_melodies/Ralph Cowell EDM Essential Melodies MIDI Vol. 2 - 4.mid'))\n",
    "\n",
    "encoder_input_data = []\n",
    "if np.array(song).shape[0] > 50:   \n",
    "    length = np.array(song).shape[0]\n",
    "    for j in range(length // seq_length):\n",
    "        encoder_input_data.append(song[seq_length*j:seq_length*(j+1)])\n",
    "print(np.array(encoder_input_data).shape)        \n",
    "        \n",
    "#encoder_input_data = get_embeded_songs(encoder_input_data, tokens, num_encoder_tokens)                                                       \n",
    "encoder_input_data1 = []\n",
    "for i, song in enumerate(encoder_input_data):\n",
    "    if(i%50==0):\n",
    "        print('Processing embed: {}/{}'.format(i,np.array(encoder_input_data).shape[0]))\n",
    "    embed_song = []\n",
    "    for i, state in enumerate(song):\n",
    "        idx = state_to_token(state, tokens)\n",
    "        embed = np.zeros(num_encoder_tokens)\n",
    "        embed[idx] = 1\n",
    "        embed_song.append(embed)\n",
    "    encoder_input_data1.append(embed_song)\n",
    "\n",
    "# Take one sequence (part of the training test)\n",
    "# for trying out decoding.\n",
    "print('[*] Encoding-Decoding')\n",
    "input_seq = encoder_input_data1[:-1]\n",
    "\n",
    "decoded_songs = []\n",
    "decoded_songs.append(decode_sequence(input_seq))\n",
    "for i in range(5):\n",
    "    decoded_songs.append(decode_sequence((\n",
    "                    np.reshape(decoded_songs[-1], \n",
    "                        (1, decoded_songs[-1].shape[0], decoded_songs[-1].shape[1])))))\n",
    "\n",
    "\n",
    "decoded_song = embed_song_to_song(np.concatenate(decoded_songs, axis = 0), tokens)\n",
    "\n",
    "# Converting Song to midi from matrix\n",
    "print('[*] Converting and saving song')\n",
    "midi_manipulation.noteStateMatrixToMidi(decoded_song, 'edm_example_40songs_20seqlength.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2937"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(decoded_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embed: 0/3\n",
      "[*] Encoding-Decoding\n",
      "Processing embed: 0/3\n",
      "[*] Encoding-Decoding\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "(2, 1, 512)\n",
      "[iter:0] [max_decoder_seq_length: 20]\n",
      "[*] Converting and saving song\n"
     ]
    }
   ],
   "source": [
    "def mixing_sequence(input_seq1, input_seq2):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value2 = encoder_model.predict(input_seq1)\n",
    "    states_value = encoder_model.predict(input_seq2)\n",
    "    \n",
    "    for i, states1 in enumerate(states_value):\n",
    "        for j, states2 in enumerate(states1):\n",
    "            for k, states3 in enumerate(states2):\n",
    "                states_value[i][j][k] = (states3*2 + states_value2[i][j][k]*1.5)/2\n",
    "\n",
    "    #print(type(states_value))\n",
    "    #states_value[:,:2,:] *= np.array(states_value2)\n",
    "    #print(np.array(states_value1).shape)\n",
    "    #for i, (state1, state2) in enumerate(zip(states_value1, states_value2)):\n",
    "    #    for j, (s2_1, s2_2) in enumerate(zip(state1, state2)):\n",
    "     #       id1 = np.argmax(s2_1)\n",
    "    #        id2 = np.argmax(s2_2)\n",
    "     #       idx = (id1+id2) // 2\n",
    "     #       states_value[i, j, idx] = 1\n",
    "      #      break\n",
    "\n",
    "        \n",
    "            \n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    song_matrix = np.zeros(\n",
    "                        (max_decoder_seq_length, \n",
    "                        num_decoder_tokens),\n",
    "                        dtype='float32')\n",
    "    i = 0\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        idx = np.argmax(output_tokens[-1,-1,:])\n",
    "        #print(output_tokens[-1,-1,:])\n",
    "        song_matrix[i, idx] = 1\n",
    "        target_seq[0, 0, idx] = 1.\n",
    "        #print(np.array(h).shape)\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (i+2 > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        \n",
    "        \n",
    "\n",
    "        if i%250 == 0:\n",
    "            print('[iter:{}] [max_decoder_seq_length: {}]'.format(i, max_decoder_seq_length))\n",
    "        i+=1\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return song_matrix\n",
    "\n",
    "seq_length = max_encoder_seq_length\n",
    "\n",
    "song = np.array(midi_manipulation.midiToNoteStateMatrix('./data/edm_essential_melodies/Ralph Cowell EDM Essential Melodies MIDI Vol. 1 - 6.mid'))\n",
    "\n",
    "encoder_input_data = []\n",
    "if np.array(song).shape[0] > 50:   \n",
    "    length = np.array(song).shape[0]\n",
    "    for j in range(length // seq_length):\n",
    "        encoder_input_data.append(song[seq_length*j:seq_length*(j+1)])\n",
    "        \n",
    "        \n",
    "#encoder_input_data = get_embeded_songs(encoder_input_data, tokens, num_encoder_tokens)                                                       \n",
    "encoder_input_data1 = []\n",
    "for i, song in enumerate(encoder_input_data):\n",
    "    if(i%50==0):\n",
    "        print('Processing embed: {}/{}'.format(i,np.array(encoder_input_data).shape[0]))\n",
    "    embed_song = []\n",
    "    for i, state in enumerate(song):\n",
    "        idx = state_to_token(state, tokens)\n",
    "        embed = np.zeros(num_encoder_tokens)\n",
    "        embed[idx] = 1\n",
    "        embed_song.append(embed)\n",
    "    encoder_input_data1.append(embed_song)\n",
    "\n",
    "# Take one sequence (part of the training test)\n",
    "# for trying out decoding.\n",
    "print('[*] Encoding-Decoding')\n",
    "input_seq1 = encoder_input_data1[:-1]\n",
    "\n",
    "\n",
    "song = np.array(midi_manipulation.midiToNoteStateMatrix('./data/edm_essential_melodies/Ralph Cowell EDM Essential Melodies MIDI Vol. 2 - 10.mid'))\n",
    "\n",
    "encoder_input_data = []\n",
    "if np.array(song).shape[0] > 50:   \n",
    "    length = np.array(song).shape[0]\n",
    "    for j in range(length // seq_length):\n",
    "        encoder_input_data.append(song[seq_length*j:seq_length*(j+1)])\n",
    "        \n",
    "        \n",
    "#encoder_input_data = get_embeded_songs(encoder_input_data, tokens, num_encoder_tokens)                                                       \n",
    "encoder_input_data1 = []\n",
    "for i, song in enumerate(encoder_input_data):\n",
    "    if(i%50==0):\n",
    "        print('Processing embed: {}/{}'.format(i,np.array(encoder_input_data).shape[0]))\n",
    "    embed_song = []\n",
    "    for i, state in enumerate(song):\n",
    "        idx = state_to_token(state, tokens)\n",
    "        embed = np.zeros(num_encoder_tokens)\n",
    "        embed[idx] = 1\n",
    "        embed_song.append(embed)\n",
    "    encoder_input_data1.append(embed_song)\n",
    "    \n",
    "# Take one sequence (part of the training test)\n",
    "# for trying out decoding.\n",
    "print('[*] Encoding-Decoding')\n",
    "input_seq2 = encoder_input_data1[:-1]\n",
    "\n",
    "decoded_songs = []\n",
    "decoded_songs.append(mixing_sequence(input_seq1, input_seq2))\n",
    "for i in range(2):\n",
    "    decoded_songs.append(decode_sequence((\n",
    "                    np.reshape(decoded_songs[-1], \n",
    "                        (1, decoded_songs[-1].shape[0], decoded_songs[-1].shape[1])))))\n",
    "\n",
    "\n",
    "decoded_song = embed_song_to_song(np.concatenate(decoded_songs, axis = 0), tokens)\n",
    "edm+=1\n",
    "# Converting Song to midi from matrix\n",
    "print('[*] Converting and saving song')\n",
    "midi_manipulation.noteStateMatrixToMidi(decoded_song, 'mixing_edm'+str(edm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((1,1,1,11432)) + np.zeros((2,1,1024)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
